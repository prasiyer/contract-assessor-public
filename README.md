# AI Contract Assessor

A Gradio-based web application for evaluating and understanding vendor contracts. Created for assisting procurement teams in evaluating supplier agreements through automated scoring, vendor based question answering, and optional metrics visualization.

## Application Overview

This application provides three core capabilities:

1. **Draft Contract Review** – Upload a supplier contract PDF to receive an LLM-generated assessment summary and parameter-level scoring based on customizable evaluation criteria.
2. **Vendor-based Q&A (RAG)** – Ask questions across your collection of executed contracts, with retrieval limited to specific vendors.
3. **Metrics Dashboard** – Visualize pre-generated analytics and charts (optional, if configured).

The application is built as proof-of-concept for using LLMs for procurement use-cases. Core assessment logic is implemented in `Code/services/contract_assessor_script.py`. RAG based chat logic is implemented in `Code/services/chat.py`

### Intended Users

- **Procurement and Sourcing Teams**: Search and retrieve specific information from executed contract repositories
- **Contract Managers**: Quickly evaluate draft supplier agreements against organizational standards. Review metrics based on qualitative and quantitative parameters extracted from collection of contracts

### Scope and Limitations

- Application has been developed by a procurement professional and meant for procurement professionals (not for lawyers).
- Real contract examples, proprietary evaluation prompts, and pre-computed vendor data are intentionally excluded from this repository.
- Users must generate their own embeddings and evaluation criteria files (see Prerequisites).

## Core Functionality

### 1. Draft Contract Review

- Upload a supplier contract PDF (validated as `.pdf`, maximum 10MB)
- The application evaluates the contract against a customizable set of questions defined in `Input/Contract_Eval_Questions_v6.csv`
- Returns:
  - Parameter-level scores for each evaluation criterion
  - AI-generated summary highlighting strengths, risks, and areas of concern
- Parameter extraction from PDF and Scoring logic are implemented in `Code/services/contract_assessor_script.py`

### 2. Vendor-based Question Answering (RAG)

- Ask natural language questions about executed supplier agreements
- Questions are scoped to specific vendors using semantic matching
- The application retrieves relevant contract chunks from pre-computed embeddings and generates answers grounded in your contract corpus
- Retrieval is powered by Azure OpenAI embeddings and similarity search
- If vendor match confidence is below the configured threshold, the system will indicate it cannot find a relevant vendor
- Logic for semantic vendor matching, RAG based retrieval are implemented in `Code/services/chat.py`

### 3. Metrics Dashboard (Optional)

- Displays pre-generated charts and analytics from the `Output/Charts/` directory
- Enables visualization of contract portfolio metrics, trends, and insights
- Requires external generation of chart files (not included in this repository)

### Core Logic Reference

Contract assessment, scoring, and retrieval logic are implemented in:
- `Code/services/contract_assessor_script.py` – Main assessment engine
- `Code/services/chat.py` – Vendor-based question answering
- `Code/services/data.py` – Data loading and embedding cache utilities

## Prerequisites

### A. Required Files (User-Generated)

This application requires several data files that **must be generated by the user** before the application can run. These files are not included in the repository.

#### 1. `Output/vendor_name_embeddings.csv`

**Purpose**: Enables vendor name matching for the question answering feature.

**Structure**: Must contain the following columns:
- `vendor_name` – Name of the vendor/supplier
- `vendor_small3_embedding` – Embedding vector (stringified array) generated using Azure OpenAI `text-embedding-3-small` or equivalent

**Notes**:
- Embeddings are parsed at runtime from stringified array format
- This file is loaded during application startup; missing file will cause startup failure

#### 2. `Output/contract_chunk_embeddings.csv`

**Purpose**: Powers the retrieval-augmented generation (RAG) system for vendor-scoped question answering.

**Structure**: Must contain the following columns:
- `vendor_name` – Name of the vendor/supplier
- `chunk_text` – Text chunk extracted from the contract
- `chunk_small3_embedding` – Embedding vector (stringified array) for the chunk

**Notes**:
- Embeddings must match the model specified in `CONTRACT_ASSISTANT_EMBEDDING_MODEL_ID`

#### 3. `Code/static_assets/scoring_rubric.py`

**Purpose**: Defines the scoring rules for quantitative and qualitative evaluation criteria used during contract assessment.

**Structure**: Python dictionary containing two main components:

1. **`qual_scoring_rubric`** – Qualitative scoring rules for YES/NO/CONDITIONAL questions:
   ```python
   {
       "qual_rules": [
           {"score": 1, "value": "NOT_SPECIFIED"},
           {"score": 1, "value": "NO"},
           {"score": 2, "value": "CONDITIONAL"},
           {"score": 3, "value": "YES"}
       ]
   }
   ```

2. **`quant_scoring_rubric`** – Quantitative scoring rules for numeric parameters (e.g., payment terms, contract duration):
   ```python
   {
       "Q18": [  # Example: Liability cap amount
           {"score": 1, "rule": "missing"},
           {"score": 2, "min": 0, "max": 4000000, "unit": "USD"},
           {"score": 3, "min": 4000000, "unit": "USD"}
       ],
       "Q19": [  # Example: Contract term length
           {"score": 1, "rule": "missing"},
           {"score": 2, "min": 0, "max": 36, "unit": "months"},
           {"score": 3, "min": 36, "unit": "months"}
       ]
       # ... additional question rules
   }
   ```

**How to Generate This File**:

1. Identify the question IDs from your `Input/Contract_Eval_Questions_v6.csv` that require quantitative scoring (e.g., Q18, Q19)
2. For each quantitative question, define scoring rules with:
   - `score`: The score value (typically 1=poor, 2=acceptable, 3=good)
   - `rule`: Use `"missing"` for score 1 when the parameter is not found in the contract
   - `min`/`max`: Numeric thresholds for the parameter value
   - `unit`: The unit of measurement (e.g., "USD", "months", "days", "%")
3. For qualitative questions (YES/NO/CONDITIONAL), use the default `qual_rules` structure
4. Save as a Python dictionary in `Code/static_assets/scoring_rubric.py`

**Notes**:
- Question IDs (e.g., "Q18", "Q19") must match the question identifiers in `Input/Contract_Eval_Questions_v6.csv`
- Scoring rules are organization-specific and should reflect your procurement standards
- The scoring rubric is referenced during contract assessment to convert extracted parameter values into scores

#### 4. Runtime and Tracking Files

The application generates and maintains several files during operation:

- `Output/output_log.txt` – Application log file (if file logging is enabled)
- `Output/oai_token_usage.csv` – Tracks Azure OpenAI API token consumption 
- `Output/contract_text_WIP.md` – Temporary file for in-process contract text extraction

These files are created automatically and do not require user setup.

### B. Provided Configuration Files (Customizable)

The following file is **included in the repository** and can be customized to match your organization's evaluation criteria.

#### `Input/Contract_Eval_Questions_v6.csv`

**Purpose**: Defines the evaluation criteria and questions for draft contract review.

**Structure**: Expected columns include (at minimum):
- `question_type` – Category of the evaluation criterion (e.g., "Payment Terms", "Termination", "Liability")
- `question_text` – The specific question or criterion to evaluate
- Additional columns may include question IDs (e.g., "Q18", "Q19") that correspond to entries in `scoring_rubric.py`

**Customization**:
- This file is **fully customizable** and controls what aspects of the contract are evaluated during Draft Contract Review
- Users can define their own:
  - Organizational standards and policies
  - Compliance requirements (legal, regulatory, industry-specific)
  - Risk assessment criteria
  - Commercial terms of interest
- Modifying this file changes the assessment questions asked and the scoring output
- Add or remove rows to adjust the scope of contract evaluation
- Ensure question IDs match those defined in `Code/static_assets/scoring_rubric.py` for quantitative questions

**Example Questions**:
- "Does the contract include a liability cap?"
- "What is the payment term in days?"
- "Is there a termination for convenience clause?"
- "What is the contract renewal term?"

### C. Azure OpenAI Configuration

This application requires access to **Azure OpenAI services**. API credentials are **not stored in code or this repository** and must be provided by the user.

#### Required Environment Variables

Create a file named `contract_assessor.env` in the repository root with the following variables:

```bash
AZURE_OPENAI_API_KEY="your-api-key-here"
AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
AZURE_OPENAI_TEXT_EMBEDDING3_KEY="your-embedding-api-key-here"

# If using o4-mini or a separate Azure OpenAI resource:
AZURE_OPENAI_O4M_API_KEY="your-o4m-api-key-here"
AZURE_OPENAI_O4M_ENDPOINT="https://your-o4m-resource.openai.azure.com/"
```

#### How to Set Up

1. Copy the example environment file:
   ```bash
   cp contract_assessor.env.example contract_assessor.env
   ```

2. Edit `contract_assessor.env` and replace placeholder values with your actual Azure OpenAI credentials

#### Required Azure OpenAI Deployments

- A GPT model deployment (e.g., `gpt-4.1-mini`, `gpt-4.1`, or `o4-mini`)
- A text embedding model deployment (e.g., `text-embedding-3-small` or `text-embedding-3-large`)

Check that your Azure OpenAI resource has these models deployed and that deployment names match your configuration (see Application Configuration Parameters below).

## Application Configuration Parameters

## Application Configuration Parameters

Most application settings are configured via environment variables defined in `Code/config.py`.

### File Paths

| Parameter | Environment Variable | Default Value | Description |
|-----------|---------------------|---------------|-------------|
| `LOG_FILE_PATH` | `CONTRACT_ASSISTANT_LOG_PATH` | `Output/output_log.txt` | Location for application log file |
| `CONTRACT_CHUNK_EMBEDDINGS_FILE_PATH` | `CONTRACT_CHUNK_EMBEDDINGS_FILE_PATH` | `Output/contract_chunk_embeddings.csv` | Pre-computed contract chunk embeddings (required for Query Assistant) |
| `VENDOR_NAME_EMBEDDINGS_FILE_PATH` | `VENDOR_NAME_EMBEDDINGS_FILE_PATH` | `Output/vendor_name_embeddings.csv` | Pre-computed vendor name embeddings (required for Query Assistant) |
| `TOKEN_USAGE_FILE_PATH` | `CONTRACT_ASSISTANT_TOKEN_USAGE_PATH` | `Output/oai_token_usage.csv` | Tracks OpenAI API token usage |
| `CONTRACT_TEXT_WIP_FILE_PATH` | `CONTRACT_ASSISTANT_TEXT_WIP_PATH` | `Output/contract_text_WIP.md` | Work-in-progress contract text extraction |
| `CONTRACT_QUESTION_FILE_PATH` | `CONTRACT_ASSISTANT_QUESTION_FILE_PATH` | `Input/Contract_Eval_Questions_v6.csv` | Contract evaluation questions for Draft Reviewer |

### Model / LLM Settings

| Parameter | Environment Variable | Default Value | Description |
|-----------|---------------------|---------------|-------------|
| `DEFAULT_ASSESSOR_MODEL_ID` | `CONTRACT_ASSESSOR_MODEL_ID` | `gpt-4.1-mini` | Model ID for contract assessment tasks |
| `CONTRACT_ASSISTANT_EMBEDDING_MODEL_ID` | `CONTRACT_ASSISTANT_EMBEDDING_MODEL_ID` | `text-embedding-3-small` | Model ID for generating embeddings |
| `CONTRACT_ASSISTANT_COMPLETION_MODEL_ID` | `CONTRACT_ASSISTANT_COMPLETION_MODEL_ID` | `gpt-4.1-mini` | Model ID for completion/chat tasks |
| `DOC_TO_IMAGE_CONVERTER` | `CONTRACT_DOC_TO_IMAGE_CONVERTER` | `fitz` | PDF to image conversion method (uses PyMuPDF) |

### Application Behavior Settings

| Parameter | Environment Variable | Default Value | Description |
|-----------|---------------------|---------------|-------------|
| `VENDOR_MATCH_CONFIDENCE_THRESHOLD` | `VENDOR_MATCH_CONFIDENCE_THRESHOLD` | `0.5` | Minimum confidence score (0-1) for vendor name matching |
| `FILE_LOGGING_ENABLED` | `CONTRACT_ASSISTANT_ENABLE_FILE_LOGGING` | `1` (enabled) | Enable/disable file logging (`1`/`true` to enable, `0`/`false` to disable) |

## Using the Application

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/prasiyer/contract_assessor.git
   cd contract_assessor
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure Azure OpenAI credentials** (see Prerequisites section above)

4. **Prepare required data files** (see Prerequisites > Required Files)

### Launch the Application

```bash
python Code/app.py
```

The Gradio interface will launch at `http://127.0.0.1:7860`.

### Draft Contract Review

1. Navigate to the **Draft Reviewer** tab in the Gradio interface
2. Enter the vendor name and contract category
3. Upload a contract PDF (max 10MB, `.pdf` format only)
4. Click **Submit**
5. Review the results:
   - Parameter-level scoring table showing evaluation against each criterion
   - AI-generated assessment summary with key findings, risks, and recommendations

The scoring criteria are defined in `Input/Contract_Eval_Questions_v6.csv` and can be customized to your organization's needs.

### Vendor-Scoped Question Answering

1. Navigate to the **Query Assistant** tab
2. Enter a question scoped to a specific vendor (e.g., "What are the payment terms for [Vendor Name]?")
3. The system will:
   - Match your query to a vendor using semantic similarity against `Output/vendor_name_embeddings.csv`
   - Retrieve relevant contract chunks from `Output/contract_chunk_embeddings.csv`
   - Generate an answer grounded in your contract corpus

**Best Practices**:
- Explicitly mention the vendor name in your question for best results
- Ask specific questions about contract terms, clauses, or obligations
- If vendor match confidence is low, the system will indicate it cannot find a relevant vendor

### Metrics Dashboard

1. Navigate to the **Metrics Dashboard** tab
2. View pre-generated charts and analytics (if configured)
3. Charts are loaded from the `Output/Charts/` directory

Note: Chart generation is not included in this repository and must be implemented separately.

## What Is Intentionally Excluded

To protect proprietary information and ensure this repository can be publicly shared, the following items are **not included**:

- **Real Contract Documents**: No actual supplier agreements, executed contracts, or contract PDFs
- **Vendor Information**: No vendor names, vendor-specific data, or business relationships
- **Proprietary Evaluation Criteria**: Scoring rubrics and assessment prompts are excluded; users must define their own in `Code/static_assets/scoring_rubric.py`
- **Pre-Computed Embeddings**: No pre-generated `vendor_name_embeddings.csv` or `contract_chunk_embeddings.csv` files
- **Azure OpenAI Credentials**: API keys and endpoints must be provided by the user
- **Internal Business Logic**: Scoring logic

Users must generate their own data files and define their own evaluation criteria based on organizational needs.

## Roadmap / Coming Soon

Future enhancements planned for this repository:

- **Synthetic Demo Data**: Example contract PDFs and pre-computed embeddings using synthetic/placeholder data for testing and demonstration
- **Data analysis capability RAG chatbot**: Tool creation and access to the chatbot for performing analysis on extracted parameters

Contributions and suggestions are welcome!
